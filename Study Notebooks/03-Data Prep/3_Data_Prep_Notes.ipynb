{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16369d71",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Data Prep](#Data-Prep)\n",
    "- [Skewness](#Skewness)\n",
    "- [Data Scaling](#Data-Scaling)\n",
    "- [Encoding](#Encoding)\n",
    "- [Outlier Removal](#Outlier-Removal)\n",
    "- [Exploratory Data Analysis](#EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd77ddf",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/dataprep.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "also commonly called **data pre-processing**, is a fundamental step in the machine learning workflow. It's essentially the process of getting the raw data ready for a machine learning or AI model.  Imagine it like cleaning and prepping ingredients before cooking a meal. One wouldn't throw raw, unwashed vegetables straight into a pot when making a stew. Data preparation is essential because it directly impacts the quality and effectiveness of the machine learning model. \"Garbage in, garbage out\" applies here, if you feed your model messy or unorganized data, you'll get unreliable results.\n",
    "\n",
    "There are several important tasks in the data preparation stage, including:\n",
    "\n",
    "-   Data Cleaning: This involves identifying and fixing errors, inconsistencies, and missing values in your data.  For instance, one might need to remove duplicate entries, deal with data outliers, correct typos, or find a way to handle data points where information is absent.\n",
    "\n",
    "-   Data Manipulation: This might involve scaling your data or transforming the data in some way, such as encoding categorical variables. Encoding transforms non-numeric data (like text categories) into a format a machine learning model can understand, often using numerical representations.\n",
    "\n",
    "-   Data Reduction: In some cases, you might have a very large dataset. Data reduction techniques like dimensionality reduction can help you identify the most important features and reduce the size of your data without losing significant information.\n",
    "\n",
    "-   Feature Engineering: While data cleaning and scaling ensures that the data is usable,  feature engineering goes a step further. It's the art of creating new features or transforming existing ones to be more informative for your machine learning model. By crafting informative features, you essentially give your model a richer understanding of the data, leading to more accurate and powerful results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9c851",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------\n",
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03202261",
   "metadata": {},
   "source": [
    "# Skewness\n",
    "\n",
    "Many machine learning or deep learning algorithms (such as Linear Regression, Logistic Regression, and Artificial Neural Networks) assume that the variable data are normally distributed (i.e. follow a Gaussian distribution) and can perform much better if the data provided to them during modeling are normally distributed.\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/rightskew.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Right-skewed Distribution: When the distribution has a long tail towards the right side, then it is known as a right-skewed or positive-skewed distribution. In the right-skewed distribution, the concentration of data points towards the right tail is more than the left tail.\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/leftskew.png\" width=\"600\">\n",
    "</div>\n",
    " \n",
    "Left-skewed Distribution: When the distribution has a long tail towards the left side, then it is known as a left-skewed or negative-skewed distribution. In the negative-skewed distribution, the concentration of data points towards the left tail is more than the right tail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89138a",
   "metadata": {},
   "source": [
    "#### Measuring Skewness\n",
    "\n",
    "Skewness measures the lack of symmetry in a distribution.\n",
    "\n",
    "    A perfectly symmetric distribution (like the normal distribution) has a skewness of 0.\n",
    "\n",
    "    If data is skewed right (tail to the right), skewness is positive.\n",
    "\n",
    "    If data is skewed left (tail to the left), skewness is negative.\n",
    "\n",
    "Formula for Skewness Coefficient\n",
    "\n",
    "The sample skewness is given by:\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/skeweq.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Where:\n",
    "- n  = number of samples\n",
    "- xi = data value\n",
    "- x¯ = sample mean\n",
    "- s  = sample standard deviation\n",
    "\n",
    "Alternatively, libraries like Pandas and SciPy use\n",
    "\n",
    "`df['feature'].skew()         # from Pandas`\n",
    "\n",
    "`scipy.stats.skew(df['feature'])  # from SciPy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c81a82",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_d4da2 th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_d4da2_row0_col0, #T_d4da2_row0_col1, #T_d4da2_row1_col0, #T_d4da2_row1_col1, #T_d4da2_row2_col0, #T_d4da2_row2_col1 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_d4da2\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_d4da2_level0_col0\" class=\"col_heading level0 col0\" >Skewness Value</th>\n",
    "      <th id=\"T_d4da2_level0_col1\" class=\"col_heading level0 col1\" >Interpretation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_d4da2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_d4da2_row0_col0\" class=\"data row0 col0\" >≈0</td>\n",
    "      <td id=\"T_d4da2_row0_col1\" class=\"data row0 col1\" >Symmetric (normal)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_d4da2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_d4da2_row1_col0\" class=\"data row1 col0\" >>0</td>\n",
    "      <td id=\"T_d4da2_row1_col1\" class=\"data row1 col1\" >Right-skewed (positive skew)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_d4da2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_d4da2_row2_col0\" class=\"data row2 col0\" ><0</td>\n",
    "      <td id=\"T_d4da2_row2_col1\" class=\"data row2 col1\" >Left-skewed (negative skew)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f513b5",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_d5957 th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_d5957_row0_col0, #T_d5957_row0_col1, #T_d5957_row1_col0, #T_d5957_row1_col1, #T_d5957_row2_col0, #T_d5957_row2_col1 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_d5957\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_d5957_level0_col0\" class=\"col_heading level0 col0\" >Absolute Value</th>\n",
    "      <th id=\"T_d5957_level0_col1\" class=\"col_heading level0 col1\" >Interpretation</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_d5957_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_d5957_row0_col0\" class=\"data row0 col0\" >< 0.5</td>\n",
    "      <td id=\"T_d5957_row0_col1\" class=\"data row0 col1\" > \tApproximately symmetric</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_d5957_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_d5957_row1_col0\" class=\"data row1 col0\" >0.5 – 1</td>\n",
    "      <td id=\"T_d5957_row1_col1\" class=\"data row1 col1\" >Moderately skewed</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_d5957_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_d5957_row2_col0\" class=\"data row2 col0\" >> 1</td>\n",
    "      <td id=\"T_d5957_row2_col1\" class=\"data row2 col1\" > \tHighly skewed</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec46857",
   "metadata": {},
   "source": [
    "#### Positive/Right Skewed Data\n",
    "\n",
    "One can use simple transformations to \"normalize\" right-skewed data:\n",
    "\n",
    "-   log transformations:\n",
    "    -   np.log(), np.log10 (but this does not deal with zeros in the data)\n",
    "    -   np.log1p(x) = log(x+1) adds 1, but can only be used for positive data,\n",
    "-   square-root transformations: np.sqrt()\n",
    "\n",
    "There are several sklearn library functions for dealing with right-skwed data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0bc4a1",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_6068a th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_6068a_row0_col0, #T_6068a_row0_col1, #T_6068a_row0_col2, #T_6068a_row0_col3, #T_6068a_row1_col0, #T_6068a_row1_col1, #T_6068a_row1_col2, #T_6068a_row1_col3, #T_6068a_row2_col0, #T_6068a_row2_col1, #T_6068a_row2_col2, #T_6068a_row2_col3, #T_6068a_row3_col0, #T_6068a_row3_col1, #T_6068a_row3_col2, #T_6068a_row3_col3, #T_6068a_row4_col0, #T_6068a_row4_col1, #T_6068a_row4_col2, #T_6068a_row4_col3 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_6068a\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_6068a_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
    "      <th id=\"T_6068a_level0_col1\" class=\"col_heading level0 col1\" >Formula / Tool</th>\n",
    "      <th id=\"T_6068a_level0_col2\" class=\"col_heading level0 col2\" >Handles Zeros?</th>\n",
    "      <th id=\"T_6068a_level0_col3\" class=\"col_heading level0 col3\" >Comment</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_6068a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_6068a_row0_col0\" class=\"data row0 col0\" >Log1p (Natural log + 1)</td>\n",
    "      <td id=\"T_6068a_row0_col1\" class=\"data row0 col1\" >np.log1p(x) = log(1 + x)</td>\n",
    "      <td id=\"T_6068a_row0_col2\" class=\"data row0 col2\" >yes</td>\n",
    "      <td id=\"T_6068a_row0_col3\" class=\"data row0 col3\" >Handles zeros and negative values</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_6068a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_6068a_row1_col0\" class=\"data row1 col0\" >Box-Cox (Scikit-Learn)</td>\n",
    "      <td id=\"T_6068a_row1_col1\" class=\"data row1 col1\" >PowerTransformer(method='box-cox')</td>\n",
    "      <td id=\"T_6068a_row1_col2\" class=\"data row1 col2\" > \tNo (requires x > 0)</td>\n",
    "      <td id=\"T_6068a_row1_col3\" class=\"data row1 col3\" >Requires positive values, can be shifted</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_6068a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_6068a_row2_col0\" class=\"data row2 col0\" >Yeo-Johnson (Scikit-Learn)</td>\n",
    "      <td id=\"T_6068a_row2_col1\" class=\"data row2 col1\" >PowerTransformer(method='yeo-johnson')</td>\n",
    "      <td id=\"T_6068a_row2_col2\" class=\"data row2 col2\" >yes</td>\n",
    "      <td id=\"T_6068a_row2_col3\" class=\"data row2 col3\" >Handles zeros and negative values</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_6068a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
    "      <td id=\"T_6068a_row3_col0\" class=\"data row3 col0\" >Custom log(x + ε)</td>\n",
    "      <td id=\"T_6068a_row3_col1\" class=\"data row3 col1\" >np.log(x + 1e-6)</td>\n",
    "      <td id=\"T_6068a_row3_col2\" class=\"data row3 col2\" >yes</td>\n",
    "      <td id=\"T_6068a_row3_col3\" class=\"data row3 col3\" >Customizable epsilon value</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_6068a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
    "      <td id=\"T_6068a_row4_col0\" class=\"data row4 col0\" >QuantileTransformer</td>\n",
    "      <td id=\"T_6068a_row4_col1\" class=\"data row4 col1\" >QuantileTransformer(output_distribution='normal')</td>\n",
    "      <td id=\"T_6068a_row4_col2\" class=\"data row4 col2\" >yes</td>\n",
    "      <td id=\"T_6068a_row4_col3\" class=\"data row4 col3\" >Transforms to normal distribution, Smooths heavy tails; robust to outliers and zero values</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ac7a97",
   "metadata": {},
   "source": [
    "#### Negative/Left Skewed Data\n",
    "\n",
    "Dealing with left-skewed data (where the tail is on the left and the bulk of values are on the right) is less commonly discussed than right-skewed data - but it’s just as important when preparing data for ML algorithms that assume symmetry or normality.\n",
    "\n",
    "The simplest way of dealing with left skewed data is to transform it to right skewed distribution, and then apply the approaches above.\n",
    "\n",
    "This can easily be done by simply:\n",
    "\n",
    "-   Flip: Multiplying the data by -1\n",
    "-   Shift: Adding the lowest value (i.e. most negative value) to the data.\n",
    "\n",
    "This will transform the negative-skewed distribution to right-skewed, starting from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2ec20",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------\n",
    "# Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee1f37",
   "metadata": {},
   "source": [
    "Data scaling is a technique for transforming the values of variables within a dataset to a similar range. In other words to convert all variables to the same order-of-magnitude.\n",
    "\n",
    "There are several reasons why scaling is important:\n",
    "\n",
    "-   Fair treatment of features: Imagine features like \"income\" (in thousands) and \"age\". Without scaling, the model would overemphasize income due to its larger magnitude. Scaling levels the playing field.\n",
    "-   Improved model convergence: Many machine learning algorithms rely on distance calculations between data points. Features with vast ranges can skew these distances, hindering the model's ability to converge on an optimal solution.\n",
    "-   Outlier detection: Standardization (a specific scaling technique) transforms data to have a mean of 0 and standard deviation of 1. Values outside a specific range (e.g., +/- 2 standard deviations) can be flagged as potential outliers.\n",
    "\n",
    "There are several common data scaling techniques that may be used.\n",
    " \n",
    "\n",
    "-   Standardization: This technique transforms variables to have a mean of 0 and standard deviation of 1. It's useful when the distribution of your data is roughly Gaussian.\n",
    "-   Normalization: This technique scales features to a specific range, often 0 to 1 or -1 to 1. It's a good choice when the data distribution is unknown or non-Gaussian.\n",
    "-   Min-Max scaling: This technique scales each feature to a specific range (e.g., 0 to 1) based on its minimum and maximum values in the dataset.\n",
    "-   Robust scaling: Robust scaling is a technique designed to handle outliers. Unlike standard scaling, which relies on mean and standard deviation (easily swayed by outliers), robust scaling uses the median and interquartile range (IQR). The median represents the data's center, and IQR reflects the spread of the middle half of the data, making them less influenced by extreme values. This ensures a more robust scaling process, especially for datasets with outliers.\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/scalingprocess.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "There are no absolute rules for choosing a scaling technique. The most appropriate technique depends on the data and the particular machine learning algorithm being used. However, here are some general guidelines:\n",
    "\n",
    "-   Use standardization for Gaussian distributed data and algorithms sensitive to feature means and variances (e.g., Support Vector Machines).\n",
    "-   Use normalization for algorithms where the data range is important (e.g., some neural networks).\n",
    "-   Use min-max scaling for a simple and quick approach, but be aware it can be sensitive to outliers.\n",
    "-   Use robust scaling if the data has outliers and there is a concern about their impact on scaling.\n",
    "-   Use robust scaling when using machine learning algorithms sensitive to feature means and variances (e.g., Support Vector Machines).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b78f29",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_682ba th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_682ba_row0_col0, #T_682ba_row0_col1, #T_682ba_row0_col2, #T_682ba_row0_col3, #T_682ba_row0_col4, #T_682ba_row0_col5, #T_682ba_row1_col0, #T_682ba_row1_col1, #T_682ba_row1_col2, #T_682ba_row1_col3, #T_682ba_row1_col4, #T_682ba_row1_col5, #T_682ba_row2_col0, #T_682ba_row2_col1, #T_682ba_row2_col2, #T_682ba_row2_col3, #T_682ba_row2_col4, #T_682ba_row2_col5, #T_682ba_row3_col0, #T_682ba_row3_col1, #T_682ba_row3_col2, #T_682ba_row3_col3, #T_682ba_row3_col4, #T_682ba_row3_col5, #T_682ba_row4_col0, #T_682ba_row4_col1, #T_682ba_row4_col2, #T_682ba_row4_col3, #T_682ba_row4_col4, #T_682ba_row4_col5, #T_682ba_row5_col0, #T_682ba_row5_col1, #T_682ba_row5_col2, #T_682ba_row5_col3, #T_682ba_row5_col4, #T_682ba_row5_col5 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_682ba\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_682ba_level0_col0\" class=\"col_heading level0 col0\" >Scaler</th>\n",
    "      <th id=\"T_682ba_level0_col1\" class=\"col_heading level0 col1\" >What It Does</th>\n",
    "      <th id=\"T_682ba_level0_col2\" class=\"col_heading level0 col2\" >Handles Outliers?</th>\n",
    "      <th id=\"T_682ba_level0_col3\" class=\"col_heading level0 col3\" >Output Range / Properties</th>\n",
    "      <th id=\"T_682ba_level0_col4\" class=\"col_heading level0 col4\" >Best Used When...</th>\n",
    "      <th id=\"T_682ba_level0_col5\" class=\"col_heading level0 col5\" >Common Use Cases</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_682ba_row0_col0\" class=\"data row0 col0\" >StandardScaler</td>\n",
    "      <td id=\"T_682ba_row0_col1\" class=\"data row0 col1\" >Standardizes features by removing the mean and scaling to unit variance</td>\n",
    "      <td id=\"T_682ba_row0_col2\" class=\"data row0 col2\" >No</td>\n",
    "      <td id=\"T_682ba_row0_col3\" class=\"data row0 col3\" >Mean = 0, Std = 1</td>\n",
    "      <td id=\"T_682ba_row0_col4\" class=\"data row0 col4\" >Features follow a normal distribution</td>\n",
    "      <td id=\"T_682ba_row0_col5\" class=\"data row0 col5\" >Logistic Regression, SVM, PCA, k-NN</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_682ba_row1_col0\" class=\"data row1 col0\" >MinMaxScaler</td>\n",
    "      <td id=\"T_682ba_row1_col1\" class=\"data row1 col1\" >Scales features to a specified range (default [0, 1])</td>\n",
    "      <td id=\"T_682ba_row1_col2\" class=\"data row1 col2\" >No</td>\n",
    "      <td id=\"T_682ba_row1_col3\" class=\"data row1 col3\" >Scales to [min, max]</td>\n",
    "      <td id=\"T_682ba_row1_col4\" class=\"data row1 col4\" >Features vary widely in scale but no strong outliers</td>\n",
    "      <td id=\"T_682ba_row1_col5\" class=\"data row1 col5\" >Neural Networks, Image data, Distance-based models</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_682ba_row2_col0\" class=\"data row2 col0\" >RobustScaler</td>\n",
    "      <td id=\"T_682ba_row2_col1\" class=\"data row2 col1\" >Scales using the median and IQR (Interquartile Range)</td>\n",
    "      <td id=\"T_682ba_row2_col2\" class=\"data row2 col2\" >Yes</td>\n",
    "      <td id=\"T_682ba_row2_col3\" class=\"data row2 col3\" >Not bounded; robust to outliers</td>\n",
    "      <td id=\"T_682ba_row2_col4\" class=\"data row2 col4\" >Features contain extreme outliers</td>\n",
    "      <td id=\"T_682ba_row2_col5\" class=\"data row2 col5\" >Financial data, Medical records</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
    "      <td id=\"T_682ba_row3_col0\" class=\"data row3 col0\" >MaxAbsScaler</td>\n",
    "      <td id=\"T_682ba_row3_col1\" class=\"data row3 col1\" >Scales features to [-1, 1] by dividing by max absolute value</td>\n",
    "      <td id=\"T_682ba_row3_col2\" class=\"data row3 col2\" >No</td>\n",
    "      <td id=\"T_682ba_row3_col3\" class=\"data row3 col3\" >Preserves sparsity; good for sparse data</td>\n",
    "      <td id=\"T_682ba_row3_col4\" class=\"data row3 col4\" >Working with sparse data</td>\n",
    "      <td id=\"T_682ba_row3_col5\" class=\"data row3 col5\" >Sparse matrices (e.g., text data)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
    "      <td id=\"T_682ba_row4_col0\" class=\"data row4 col0\" >QuantileTransformer</td>\n",
    "      <td id=\"T_682ba_row4_col1\" class=\"data row4 col1\" >Maps data to a uniform or normal distribution via quantiles</td>\n",
    "      <td id=\"T_682ba_row4_col2\" class=\"data row4 col2\" >Yes</td>\n",
    "      <td id=\"T_682ba_row4_col3\" class=\"data row4 col3\" >Uniform or Gaussian distribution</td>\n",
    "      <td id=\"T_682ba_row4_col4\" class=\"data row4 col4\" >Need uniform or normal distribution explicitly</td>\n",
    "      <td id=\"T_682ba_row4_col5\" class=\"data row4 col5\" >Non-linear models, Outlier-rich data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_682ba_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
    "      <td id=\"T_682ba_row5_col0\" class=\"data row5 col0\" >PowerTransformer (Box-Cox, Yeo-Johnson)</td>\n",
    "      <td id=\"T_682ba_row5_col1\" class=\"data row5 col1\" >Stabilizes variance and makes data more Gaussian-like</td>\n",
    "      <td id=\"T_682ba_row5_col2\" class=\"data row5 col2\" >Yes</td>\n",
    "      <td id=\"T_682ba_row5_col3\" class=\"data row5 col3\" >Mean ≈ 0, Std ≈ 1</td>\n",
    "      <td id=\"T_682ba_row5_col4\" class=\"data row5 col4\" >Want to stabilize variance and make data normal</td>\n",
    "      <td id=\"T_682ba_row5_col5\" class=\"data row5 col5\" >Gaussian-sensitive models like LDA, PCA</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4357202",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------\n",
    "# Encoding\n",
    "\n",
    "<style type=\"text/css\">\n",
    "#T_78532 th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_78532_row0_col0, #T_78532_row0_col1, #T_78532_row0_col2, #T_78532_row1_col0, #T_78532_row1_col1, #T_78532_row1_col2, #T_78532_row2_col0, #T_78532_row2_col1, #T_78532_row2_col2 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_78532\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_78532_level0_col0\" class=\"col_heading level0 col0\" >Feature Type</th>\n",
    "      <th id=\"T_78532_level0_col1\" class=\"col_heading level0 col1\" >Example</th>\n",
    "      <th id=\"T_78532_level0_col2\" class=\"col_heading level0 col2\" >Encoding Strategy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_78532_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_78532_row0_col0\" class=\"data row0 col0\" >Nominal</td>\n",
    "      <td id=\"T_78532_row0_col1\" class=\"data row0 col1\" >Petrol, Diesel, CNG</td>\n",
    "      <td id=\"T_78532_row0_col2\" class=\"data row0 col2\" >One-Hot</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_78532_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_78532_row1_col0\" class=\"data row1 col0\" >Ordinal</td>\n",
    "      <td id=\"T_78532_row1_col1\" class=\"data row1 col1\" >Small, Medium, Large</td>\n",
    "      <td id=\"T_78532_row1_col2\" class=\"data row1 col2\" >Label / Ordinal Map</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_78532_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_78532_row2_col0\" class=\"data row2 col0\" >Binary</td>\n",
    "      <td id=\"T_78532_row2_col1\" class=\"data row2 col1\" >Yes, No</td>\n",
    "      <td id=\"T_78532_row2_col2\" class=\"data row2 col2\" >Map to 0/1</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32795aa9",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------\n",
    "# Outlier Removal\n",
    "\n",
    "Outliers are data points that fall significantly outside the typical range of your data.  They can distort a machine learning model's results and lead to inaccurate predictions. There are two main approaches to dealing with outliers: \n",
    "\n",
    "-   Removal:  This involves simply removing the outlier data from the dataset.\n",
    "-   Imputation: This involves replacing the outlier values with substituted values.\n",
    "\n",
    "\n",
    "There are several reasons why one might want to remove outliers from a dataset. These include:\n",
    "\n",
    "-   Distorted models: Outliers can significantly influence the model's learning process, causing it to overfit to the extreme values instead of capturing the underlying pattern.\n",
    "-   Reduced accuracy: Models trained on data with outliers might perform well on the training data but fail to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c8a8b1",
   "metadata": {},
   "source": [
    "## Interquartile Range (IQR) Approach\n",
    "\n",
    "Here we describe the process of interquartile range outlier removal. There are other approaches, but we won't discuss them in this course.\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/IQR.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "The Interquartile Range (IQR) is a statistical measure used to understand the spread of data, specifically focusing on the middle half of your dataset. It tells you how much variability exists within the central 50% of your data points, after ordering them from least to greatest.\n",
    "\n",
    "Imagine dividing your ordered data into four equal parts. The points that divide these parts are called quartiles. There are three quartiles (Q1, Q2, and Q3).\n",
    "\n",
    "-   Q1 (first quartile): Represents the value where 25% of the data falls below it and 75% falls above.\n",
    "-   Q2 (second quartile): This is the median of your data, the middle value when ordered.\n",
    "-   Q3 (third quartile): Represents the value where 75% of the data falls below it and 25% falls above.\n",
    "\n",
    "\n",
    "Interquartile Range (IQR):  This is simply the difference between the third quartile (Q3) and the first quartile (Q1). So,\n",
    "\n",
    "IQR=Q3−Q1.\n",
    "\n",
    "In layman's terms, the IQR tells us how spread out the data is.\n",
    "\n",
    "One can remove outliers that fall beyond a chosen multiple of the IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code\n",
    "# the following function will remove outliers outside 1.5 times the IQR:\n",
    "\n",
    "def remove_outliers_iqr(df, columns):\n",
    "    df_clean = df.copy()\n",
    "    for col in columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]\n",
    "    return df_clean\n",
    "\n",
    "numeric_columns = df.select_dtypes(include='number').columns.tolist()\n",
    "df_cleaned = remove_outliers_iqr(df, numeric_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd7a65",
   "metadata": {},
   "source": [
    "## The z-score Method\n",
    "\n",
    "In statistics, the z-score (also known as the standard score) tells you how many standard deviations a data point is from the mean of the distribution. The interpretati0n of the z-score is summarized in the following table:\n",
    "\n",
    "<style type=\"text/css\">\n",
    "#T_9e2df th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_9e2df_row0_col0, #T_9e2df_row0_col1, #T_9e2df_row1_col0, #T_9e2df_row1_col1, #T_9e2df_row2_col0, #T_9e2df_row2_col1, #T_9e2df_row3_col0, #T_9e2df_row3_col1 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_9e2df\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_9e2df_level0_col0\" class=\"col_heading level0 col0\" >Z-score</th>\n",
    "      <th id=\"T_9e2df_level0_col1\" class=\"col_heading level0 col1\" >Meaning</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_9e2df_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_9e2df_row0_col0\" class=\"data row0 col0\" >0</td>\n",
    "      <td id=\"T_9e2df_row0_col1\" class=\"data row0 col1\" >Data point is exactly at the mean</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_9e2df_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_9e2df_row1_col0\" class=\"data row1 col0\" >+1</td>\n",
    "      <td id=\"T_9e2df_row1_col1\" class=\"data row1 col1\" >1 standard deviation above the mean</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_9e2df_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_9e2df_row2_col0\" class=\"data row2 col0\" >-1</td>\n",
    "      <td id=\"T_9e2df_row2_col1\" class=\"data row2 col1\" >1 standard deviation below the mean</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_9e2df_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
    "      <td id=\"T_9e2df_row3_col0\" class=\"data row3 col0\" >> +3 or < -3</td>\n",
    "      <td id=\"T_9e2df_row3_col1\" class=\"data row3 col1\" >Potential outlier (extreme deviation)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8ceb1",
   "metadata": {},
   "source": [
    "#### z-Score and the Empirical Rule\n",
    "\n",
    "The Empirical Rule (also called the 68–95–99.7 Rule) for a normal distribution, which is the foundation for the z-score approach to outlier detection is illustrated by the following diagram:\n",
    "\n",
    "<div>\n",
    "    <img src=\"../src/images/Zscore.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "The z-score measures how many standard deviations (s or SD in the figure) a data point is from the mean (x¯ or M above ). In the normal distribution shown:\n",
    "\n",
    "<style type=\"text/css\">\n",
    "#T_c544f th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_c544f_row0_col0, #T_c544f_row0_col1, #T_c544f_row0_col2, #T_c544f_row0_col3, #T_c544f_row1_col0, #T_c544f_row1_col1, #T_c544f_row1_col2, #T_c544f_row1_col3, #T_c544f_row2_col0, #T_c544f_row2_col1, #T_c544f_row2_col2, #T_c544f_row2_col3 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_c544f\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_c544f_level0_col0\" class=\"col_heading level0 col0\" >Range</th>\n",
    "      <th id=\"T_c544f_level0_col1\" class=\"col_heading level0 col1\" >Z-Score Range</th>\n",
    "      <th id=\"T_c544f_level0_col2\" class=\"col_heading level0 col2\" >% of Data</th>\n",
    "      <th id=\"T_c544f_level0_col3\" class=\"col_heading level0 col3\" >Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_c544f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_c544f_row0_col0\" class=\"data row0 col0\" >±1s</td>\n",
    "      <td id=\"T_c544f_row0_col1\" class=\"data row0 col1\" >z ∈ [−1, +1]</td>\n",
    "      <td id=\"T_c544f_row0_col2\" class=\"data row0 col2\" >68%</td>\n",
    "      <td id=\"T_c544f_row0_col3\" class=\"data row0 col3\" >Most data lies within 1 standard deviation of the mean</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_c544f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_c544f_row1_col0\" class=\"data row1 col0\" >±2s</td>\n",
    "      <td id=\"T_c544f_row1_col1\" class=\"data row1 col1\" >z ∈ [−2, +2]</td>\n",
    "      <td id=\"T_c544f_row1_col2\" class=\"data row1 col2\" >95%</td>\n",
    "      <td id=\"T_c544f_row1_col3\" class=\"data row1 col3\" >Covers almost all typical values</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_c544f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_c544f_row2_col0\" class=\"data row2 col0\" >±3s</td>\n",
    "      <td id=\"T_c544f_row2_col1\" class=\"data row2 col1\" >z ∈ [−3, +3]</td>\n",
    "      <td id=\"T_c544f_row2_col2\" class=\"data row2 col2\" >99.7%</td>\n",
    "      <td id=\"T_c544f_row2_col3\" class=\"data row2 col3\" >Includes nearly all data in a normal distribution</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "This means that data points with |z| > 3 fall in the outer 0.3% (i.e., beyond ±3s). These are considered extreme outliers in a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4837eb",
   "metadata": {},
   "source": [
    "#### When to use z-score method\n",
    "\n",
    "Care should be taken when using the z-score method. It is appropriate when:\n",
    "-   Data is approximately normally distributed\n",
    "-   A simple, fast method is required\n",
    "\n",
    "It is not ideal in cases where:\n",
    "-   Data is heavily skewed or contains non-Gaussian outliers\n",
    "-   You're working with multivariate data (consider use Isolation Forest instead)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b56a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code\n",
    "# For example, the following python code will remove all data points outside 3 standard deviations of the mean.\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "z_scores = df[numeric_columns].apply(zscore)\n",
    "df_cleaned = df[(abs(z_scores) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79587635",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    "#T_b15e8 th {\n",
    "  text-align: left;\n",
    "}\n",
    "#T_b15e8_row0_col0, #T_b15e8_row0_col1, #T_b15e8_row0_col2, #T_b15e8_row0_col3, #T_b15e8_row0_col4, #T_b15e8_row1_col0, #T_b15e8_row1_col1, #T_b15e8_row1_col2, #T_b15e8_row1_col3, #T_b15e8_row1_col4, #T_b15e8_row2_col0, #T_b15e8_row2_col1, #T_b15e8_row2_col2, #T_b15e8_row2_col3, #T_b15e8_row2_col4, #T_b15e8_row3_col0, #T_b15e8_row3_col1, #T_b15e8_row3_col2, #T_b15e8_row3_col3, #T_b15e8_row3_col4, #T_b15e8_row4_col0, #T_b15e8_row4_col1, #T_b15e8_row4_col2, #T_b15e8_row4_col3, #T_b15e8_row4_col4, #T_b15e8_row5_col0, #T_b15e8_row5_col1, #T_b15e8_row5_col2, #T_b15e8_row5_col3, #T_b15e8_row5_col4 {\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "<table id=\"T_b15e8\">\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th class=\"blank level0\" >&nbsp;</th>\n",
    "      <th id=\"T_b15e8_level0_col0\" class=\"col_heading level0 col0\" >Method</th>\n",
    "      <th id=\"T_b15e8_level0_col1\" class=\"col_heading level0 col1\" >sklearn</th>\n",
    "      <th id=\"T_b15e8_level0_col2\" class=\"col_heading level0 col2\" >Technique</th>\n",
    "      <th id=\"T_b15e8_level0_col3\" class=\"col_heading level0 col3\" >Works On</th>\n",
    "      <th id=\"T_b15e8_level0_col4\" class=\"col_heading level0 col4\" >Notes</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
    "      <td id=\"T_b15e8_row0_col0\" class=\"data row0 col0\" >IQR Method</td>\n",
    "      <td id=\"T_b15e8_row0_col1\" class=\"data row0 col1\" >None</td>\n",
    "      <td id=\"T_b15e8_row0_col2\" class=\"data row0 col2\" >Removes values beyond 1.5×IQR outside Q1/Q3</td>\n",
    "      <td id=\"T_b15e8_row0_col3\" class=\"data row0 col3\" >Univariate</td>\n",
    "      <td id=\"T_b15e8_row0_col4\" class=\"data row0 col4\" >Simple, fast, interpretable</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
    "      <td id=\"T_b15e8_row1_col0\" class=\"data row1 col0\" >Z-score Method</td>\n",
    "      <td id=\"T_b15e8_row1_col1\" class=\"data row1 col1\" >None</td>\n",
    "      <td id=\"T_b15e8_row1_col2\" class=\"data row1 col2\" >Removes values with z-score > 3 (or threshold)</td>\n",
    "      <td id=\"T_b15e8_row1_col3\" class=\"data row1 col3\" >Univariate</td>\n",
    "      <td id=\"T_b15e8_row1_col4\" class=\"data row1 col4\" >Assumes normality</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
    "      <td id=\"T_b15e8_row2_col0\" class=\"data row2 col0\" >Isolation Forest</td>\n",
    "      <td id=\"T_b15e8_row2_col1\" class=\"data row2 col1\" >sklearn.ensemble.IsolationForest</td>\n",
    "      <td id=\"T_b15e8_row2_col2\" class=\"data row2 col2\" >Tree-based anomaly detection</td>\n",
    "      <td id=\"T_b15e8_row2_col3\" class=\"data row2 col3\" >Multivariate</td>\n",
    "      <td id=\"T_b15e8_row2_col4\" class=\"data row2 col4\" >Handles high-dimensional data</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
    "      <td id=\"T_b15e8_row3_col0\" class=\"data row3 col0\" >Local Outlier Factor (LOF)</td>\n",
    "      <td id=\"T_b15e8_row3_col1\" class=\"data row3 col1\" >sklearn.neighbors.LocalOutlierFactor</td>\n",
    "      <td id=\"T_b15e8_row3_col2\" class=\"data row3 col2\" >Detects density-based local outliers</td>\n",
    "      <td id=\"T_b15e8_row3_col3\" class=\"data row3 col3\" >Multivariate</td>\n",
    "      <td id=\"T_b15e8_row3_col4\" class=\"data row3 col4\" >Good for non-globular clusters</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
    "      <td id=\"T_b15e8_row4_col0\" class=\"data row4 col0\" >DBSCAN</td>\n",
    "      <td id=\"T_b15e8_row4_col1\" class=\"data row4 col1\" >sklearn.cluster.DBSCAN</td>\n",
    "      <td id=\"T_b15e8_row4_col2\" class=\"data row4 col2\" >Clustering + noise labeling</td>\n",
    "      <td id=\"T_b15e8_row4_col3\" class=\"data row4 col3\" >Multivariate</td>\n",
    "      <td id=\"T_b15e8_row4_col4\" class=\"data row4 col4\" >Detects outliers as noise</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th id=\"T_b15e8_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
    "      <td id=\"T_b15e8_row5_col0\" class=\"data row5 col0\" >Elliptic Envelope</td>\n",
    "      <td id=\"T_b15e8_row5_col1\" class=\"data row5 col1\" >sklearn.covariance.EllipticEnvelope</td>\n",
    "      <td id=\"T_b15e8_row5_col2\" class=\"data row5 col2\" >Fits a Gaussian “envelope” around data</td>\n",
    "      <td id=\"T_b15e8_row5_col3\" class=\"data row5 col3\" >Multivariate</td>\n",
    "      <td id=\"T_b15e8_row5_col4\" class=\"data row5 col4\" >Assumes Gaussian distribution</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee1d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Code - Isolation Forest\n",
    "# For example, one can use the sklearn Isolation forest function to remove outliers as follows:\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.05, random_state=42)\n",
    "outliers = iso.fit_predict(df[numeric_columns])\n",
    "df_cleaned = df[outliers == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b3ff32",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------\n",
    "# EDA\n",
    "\n",
    "Data preparation (or pre-processing) and exploratory data analysis (EDA) are crucial steps in a machine learning project, but they serve distinct purposes.\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "\n",
    "The goal of data preparation is to get the data ready for modeling by cleaning, organizing, and formatting it. The typical tasks in data preparation may include\"\n",
    "\n",
    "Cleaning: This involves fixing errors, handling missing values, and removing inconsistencies.\n",
    "Formatting: Ensure data is in a format usable by machine learning algorithms. This may involve converting data types or scaling values.\n",
    "Labeling: If applicable, assign labels to data points for supervised learning.\n",
    "\n",
    "#### Exploratory Data Analysis (EDA)\n",
    "\n",
    "\n",
    "The goal of exploratory data analysis is to understand the characteristics of your data and uncover patterns or relationships. This may involve:\n",
    "\n",
    "Summarizing:  Get a basic understanding of the data through measures like central tendency (mean, median) and spread (standard deviation).\n",
    "Visualizing: Create charts and graphs to see trends, identify outliers, and explore relationships between variables.\n",
    "Hypothesis Generation: Based on your findings, formulate initial questions or predictions about the data.\n",
    "\n",
    "\n",
    "#### Key Differences\n",
    "\n",
    "Imagine data preparation like cleaning and organizing your ingredients for a recipe (the machine learning model). EDA is like reviewing the recipe itself, understanding the ingredients and how they interact, before you start cooking. The key differences are\n",
    "\n",
    "-   Focus: Data preparation focuses on making the data usable, while EDA focuses on understanding its content and relationships.\n",
    "-   Outcome: Data preparation creates a clean dataset, while EDA generates insights and potential hypotheses about the data.\n",
    "-   Timing: Preparation typically happens before EDA, but they can be iterative. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
