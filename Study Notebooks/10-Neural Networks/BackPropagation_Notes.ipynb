{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5eeae6",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "key algorithm that allows ANNs to learn by adjusting their weights to minimize errors.\n",
    "\n",
    "is a method usedto update the weights in a neural network based on the error of the output. It uses gradient descent to optimize the model by minimizing a loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41566d5",
   "metadata": {},
   "source": [
    "### Steps \n",
    "\n",
    "1. Forward Pass: Compute the output of the network using current weights.\n",
    "\n",
    "2. Loss Calculation: Measure how far the output is from the expected output (using a loss function like MSE or cross-entropy).\n",
    "\n",
    "3. Backward Pass:The algorithm then goes backwards through the network and:\n",
    "o Calculates how much each weight contributed to the error (using partial derivatives).\n",
    "o Propagate this error backwards through the network layer-by-layer.\n",
    "\n",
    "4. Weight Update: Adjust weights to reduce the error using gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf81c53",
   "metadata": {},
   "source": [
    "### Learning Epochs\n",
    "\n",
    "• Forward Pass: Each data point (or a batch of data points) in the training set is fed forward through the network. This means the input data travels through the layers of neurons, undergoing calculations at each step. The network makes predictions (i.e. calculated the output) based on the current weights and biases.Initially, these predictions are likely to be quite inaccurate since the inital weights and biases are randomly chosen.\n",
    "\n",
    "• Calculate Loss: The error between the predictions (i.e. the network output) and the actual target values (i.e. the yy is calculated using a loss function. This tells us how poorly the network is performing.\n",
    "\n",
    "• Backward Pass: The network's weights and biases are adjusted using an optimization algorithm (like gradient descent in our backpropagation algorithm) to minimize this error. The goal is to make the network's predictions more accurate in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e928629a",
   "metadata": {},
   "source": [
    "### Hyperparameter \n",
    "\n",
    "• Too few epochs might result in underfitting, where the network hasn't learned the underlying patterns.\n",
    "\n",
    "• Too many epochs can lead to overfitting, where the network learns the training data too well, including its noise, and performs poorly on unseen data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
